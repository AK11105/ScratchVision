{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260965cf-7811-45ed-bb5e-8c02d3c66f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af3a3f-6431-49c2-b4a1-ab82e338d114",
   "metadata": {},
   "source": [
    "## ReLU Non-Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e3f169-8979-4638-91c1-7a8b2f0839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reLU = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6e9d-8a4c-4f90-ad53-fd64d42baf0b",
   "metadata": {},
   "source": [
    "## Local Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac217891-375c-428d-bde8-6b145756a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn = nn.LocalResponseNorm(\n",
    "    size=5,\n",
    "    alpha=1e-4,\n",
    "    beta=0.75,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29fb2a-64e7-4129-8634-a2228d458949",
   "metadata": {},
   "source": [
    "## Overlapping Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c71536-70b5-480b-902e-1fe980df47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ompool = nn.MaxPool2d(kernel_size=3, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f3ec0-0886-44ec-aa9e-b564f6d855cb",
   "metadata": {},
   "source": [
    "## Complete Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65d91ea-20cd-4c56-a834-060b77855e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                activation,\n",
    "                pool,\n",
    "                norm,\n",
    "                dropout,\n",
    "                C1_in=3, C1_out=96, C1_kernel=11, C1_stride=4, C1_padding=2,\n",
    "                C2_in=96, C2_out=256, C2_kernel=5, C2_stride=1, C2_padding=2,\n",
    "                C3_in=256, C3_out=384, C3_kernel=3, C3_stride=1, C3_padding=1,\n",
    "                C4_in=384, C4_out=384, C4_kernel=3, C4_stride=1, C4_padding=1,\n",
    "                C5_in=384, C5_out=256, C5_kernel=3, C5_stride=1, C5_padding=1,\n",
    "                FC1_in=9216, FC1_out=4096,\n",
    "                FC2_in=4096, FC2_out=4096,\n",
    "                FC3_in=4096, FC3_out=1000):\n",
    "        super().__init__()\n",
    "        self.activation = activation or nn.ReLU()\n",
    "        self.pool = pool or nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.norm = norm or nn.LocalResponseNorm(k=2, alpha=1e-4, beta=0.75, size=5)\n",
    "        self.dropout = dropout or nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=C1_in,\n",
    "            out_channels=C1_out,\n",
    "            kernel_size=C1_kernel,\n",
    "            stride=C1_stride,\n",
    "            padding=C1_padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=C2_in,\n",
    "            out_channels=C2_out,\n",
    "            kernel_size=C2_kernel,\n",
    "            stride=C2_stride,\n",
    "            padding=C2_padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=C3_in,\n",
    "            out_channels=C3_out,\n",
    "            kernel_size=C3_kernel,\n",
    "            stride=C3_stride,\n",
    "            padding=C3_padding\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=C4_in,\n",
    "            out_channels=C4_out,\n",
    "            kernel_size=C4_kernel,\n",
    "            stride=C4_stride,\n",
    "            padding=C4_padding\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=C5_in,\n",
    "            out_channels=C5_out,\n",
    "            kernel_size=C5_kernel,\n",
    "            stride=C5_stride,\n",
    "            padding=C5_padding\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=FC1_in, out_features=FC1_out)\n",
    "        self.fc2 = nn.Linear(in_features=FC2_in, out_features=FC2_out)\n",
    "        self.fc3 = nn.Linear(in_features=FC3_in, out_features=FC3_out)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    # conv1 and conv3 get bias = 0\n",
    "                    if m is self.conv1 or m is self.conv3:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                    else:\n",
    "                        nn.init.constant_(m.bias, 1)\n",
    "    \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.norm(X)\n",
    "        X = self.pool(X)\n",
    "\n",
    "        X = self.conv2(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.norm(X)\n",
    "        X = self.pool(X)\n",
    "\n",
    "        X = self.conv3(X)\n",
    "        X = self.activation(X)\n",
    "\n",
    "        X = self.conv4(X)\n",
    "        X = self.activation(X)\n",
    "\n",
    "        X = self.conv5(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.pool(X)\n",
    "\n",
    "        X = X.view(X.size(0), -1)\n",
    "\n",
    "        X = self.fc1(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        X = self.fc2(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        X = self.fc3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e0a63-445c-4ebe-adc9-4de6e97cf84b",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4225024e-75ac-4e77-b148-e05b241e4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6989bf7c-5d99-4c25-b784-5ef495434ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightingPCA(object):\n",
    "    def __init__(self, alpha_std=0.1):\n",
    "        self.alpha_std = alpha_std\n",
    "        self.eigvals = torch.tensor([0.2175, 0.0188, 0.0045])\n",
    "        self.eigvecs = torch.tensor([\n",
    "            [-0.5675,  0.7192,  0.4009],\n",
    "            [-0.5808, -0.0045, -0.8140],\n",
    "            [-0.5836, -0.6948,  0.4203]\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        img: Tensor assumed to be shape (C, H, W), float in [0, 1].\n",
    "        \"\"\"\n",
    "        if self.alpha_std == 0:\n",
    "            return img\n",
    "\n",
    "        # Sample random alpha from N(0, 0.1)\n",
    "        alpha = torch.normal(mean=0.0, std=self.alpha_std, size=(3,))\n",
    "\n",
    "        # Compute RGB noise\n",
    "        rgb = (self.eigvecs @ (alpha * self.eigvals)).view(3, 1, 1)\n",
    "\n",
    "        return img + rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e92654-ebab-4bc4-88fe-47b10e76d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    LightingPCA(alpha_std=0.01)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.TenCrop(224),  # returns 10 PIL images\n",
    "    transforms.Lambda(lambda crops: torch.stack([\n",
    "        transforms.ToTensor()(crop) for crop in crops\n",
    "    ]))  # shape (10, 3, 224, 224)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2ff7216-fb17-43ef-8be4-c0d08c2bb447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 3, 224, 224])\n",
      "Output: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "model = AlexNet(\n",
    "    activation=None,\n",
    "    pool=None,\n",
    "    norm=None,\n",
    "    dropout=None\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(\"Input:\", x.shape)\n",
    "print(\"Output:\", out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScratchVision",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
